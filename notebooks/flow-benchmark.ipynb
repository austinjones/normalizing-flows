{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook generates, trains, and benchmarks normalizing flows on univariate, analytic distribution.\n",
    "# The source distribution can be Laplace, Exponential, Cauchy, ContinuousBernoulliContinuousBernoulli, HalfCauchy, HalfNormal, Normal, or Uniform.\n",
    "# See flows.py for a selection of modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import *\n",
    "from flows import *\n",
    "\n",
    "# see kl-estimator.ipynb for an explanation\n",
    "def kl_estimate_log(log_px, log_qx, n):\n",
    "    return (log_px - log_qx).mean()\n",
    "\n",
    "\n",
    "dim = 1\n",
    "datapoints = 2500\n",
    "\n",
    "# Any of these distributions can be used to test a flow\n",
    "# Uncomment one and run the rest of the notebook.\n",
    "# dist = Laplace(torch.tensor([0.0]), torch.tensor([1.]))\n",
    "dist = Exponential(torch.tensor([1.0]))\n",
    "# dist = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "# dist = ContinuousBernoulli(torch.tensor([0.3]))\n",
    "# dist = HalfCauchy(torch.tensor([1.0]))\n",
    "# dist = HalfNormal(torch.tensor([1.0]))\n",
    "# dist = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "# dist = TransformedDistribution { ... }\n",
    "# dist = Uniform(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "\n",
    "table = dist.sample_n(datapoints)\n",
    "dataset = torch.utils.data.TensorDataset(table)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the trainable layers.  The output is interpreted as a normally distributed variable.\n",
    "normal_flows = Flows(\n",
    "    DenseTriangularFlow(dim, True),\n",
    "    DenseTriangularFlow(dim, False),\n",
    "    DenseTriangularFlow(dim, True),\n",
    "    DenseTriangularFlow(dim, False),\n",
    "    SoftlogFlow(),\n",
    "    DenseTriangularFlow(dim, True),\n",
    "    DenseTriangularFlow(dim, False),\n",
    "    DenseTriangularFlow(dim, True),\n",
    "    DenseTriangularFlow(dim, False),\n",
    "    SoftlogFlow(),\n",
    "    DenseTriangularFlow(dim, True),\n",
    "    DenseTriangularFlow(dim, False),\n",
    "    DenseTriangularFlow(dim, True),\n",
    "    DenseTriangularFlow(dim, False),\n",
    "    SoftlogFlow()\n",
    ")\n",
    "\n",
    "# NegLogLikelihoodLoss implements the NLL on a multivariate gaussian with unit covariance\n",
    "net = FlowModule(normal_flows, NegLogLikelihoodLoss(dim))\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "final loss: 1.153\n"
    }
   ],
   "source": [
    "# Training loop!\n",
    "final_loss = 0.0\n",
    "for batch in range(200):  #1oop over the dataset multiple times\n",
    "    for _, data in enumerate(dataloader, 0):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        loss = net(data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        final_loss = loss\n",
    "print(\"final loss: %.03f\" % final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "DenseTriangularFlow()\n  w = tensor([[2.3088]])\n  b = tensor([-0.0131])\n\nDenseTriangularFlow()\n  w = tensor([[2.2353]])\n  b = tensor([-0.0088])\n\nDenseTriangularFlow()\n  w = tensor([[2.1605]])\n  b = tensor([-0.0095])\n\nDenseTriangularFlow()\n  w = tensor([[2.0920]])\n  b = tensor([-0.0129])\n\nSoftlogFlow()\n\nDenseTriangularFlow()\n  w = tensor([[1.1157]])\n  b = tensor([0.0621])\n\nDenseTriangularFlow()\n  w = tensor([[1.0999]])\n  b = tensor([0.0458])\n\nDenseTriangularFlow()\n  w = tensor([[1.0835]])\n  b = tensor([0.0316])\n\nDenseTriangularFlow()\n  w = tensor([[1.0674]])\n  b = tensor([0.0193])\n\nSoftlogFlow()\n\nDenseTriangularFlow()\n  w = tensor([[1.1526]])\n  b = tensor([-1.0044])\n\nDenseTriangularFlow()\n  w = tensor([[1.5396]])\n  b = tensor([-0.8566])\n\nDenseTriangularFlow()\n  w = tensor([[1.7632]])\n  b = tensor([-0.6966])\n\nDenseTriangularFlow()\n  w = tensor([[1.8620]])\n  b = tensor([-0.5534])\n\nSoftlogFlow()\n\n"
    }
   ],
   "source": [
    "# Prints the network parameters\n",
    "for f in normal_flows.flows:\n",
    "    print(f)\n",
    "    for name, param in f.named_parameters():\n",
    "        print(\"  %s = %s\" % (name, param.data))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0.0219, 0.0199],\n        [0.3832, 0.4190],\n        [0.5378, 0.7401],\n        ...,\n        [0.6793, 0.6549],\n        [0.5978, 0.7021],\n        [0.0302, 0.0235]], grad_fn=<StackBackward>)\n\n~kl div: tensor(0.1683, grad_fn=<MeanBackward0>)\n"
    }
   ],
   "source": [
    "# Generates a random set of points from `dist`, and tests the network performance\n",
    "# Shows side-by-side probability values (left is actual P(x))\n",
    "# Also estimates the KL divergence between the P(x), and the network's reconstruction\n",
    "\n",
    "n = 10000\n",
    "points = dist.sample_n(n)\n",
    "p_points = dist.log_prob(points)\n",
    "\n",
    "normalized_points, log_det = normal_flows(points)\n",
    "dist_target = MultivariateNormal(torch.zeros(dim), torch.eye(dim))\n",
    "p_normalized_points = dist_target.log_prob(normalized_points) + log_det\n",
    "\n",
    "uniform_with_y = torch.stack([p_points.exp().squeeze(), p_normalized_points.exp()], dim=1)\n",
    "print(uniform_with_y)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"~kl div:\", kl_estimate_log(p_points, p_normalized_points, n))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}