{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook tests normalizing flow implementations.\n",
    "# Each flow is checked for:\n",
    "# - `backwards` implementation accuracy.\n",
    "# - jacobian determinant accuracy.  this code can check that the jacobian is actually triangular (necessary for log_det to reduce to J.diag().log().sum()),\n",
    "#     and that the flow's log_det is correct\n",
    "# This is going to be the basis for the unit tests when flows.py becomes a real library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as grad\n",
    "\n",
    "def jacobian(f, x):\n",
    "    \"\"\"Computes the Jacobian of f w.r.t x.\n",
    "\n",
    "    This is according to the reverse mode autodiff rule,\n",
    "\n",
    "    sum_i v^b_i dy^b_i / dx^b_j = sum_i x^b_j R_ji v^b_i,\n",
    "\n",
    "    where:\n",
    "    - b is the batch index from 0 to B - 1\n",
    "    - i, j are the vector indices from 0 to N-1\n",
    "    - v^b_i is a \"test vector\", which is set to 1 column-wise to obtain the correct\n",
    "        column vectors out ot the above expression.\n",
    "\n",
    "    :param f: function R^N -> R^N\n",
    "    :param x: torch.tensor of shape [B, N]\n",
    "    :return: Jacobian matrix (torch.tensor) of shape [B, N, N]\n",
    "    \"\"\"\n",
    "\n",
    "    B, N = x.shape\n",
    "    y, _ = f(x)\n",
    "    # y = f(x)\n",
    "    jacobian = list()\n",
    "    for i in range(N):\n",
    "        v = torch.zeros_like(y)\n",
    "        v[:, i] = 1.\n",
    "        dy_i_dx = torch.autograd.grad(y,\n",
    "                       x,\n",
    "                       grad_outputs=v,\n",
    "                       retain_graph=True,\n",
    "                       create_graph=True,\n",
    "                       allow_unused=True)[0]  # shape [B, N]\n",
    "        jacobian.append(dy_i_dx)\n",
    "\n",
    "    jacobian = torch.stack(jacobian, dim=2).requires_grad_()\n",
    "\n",
    "    return jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as grad\n",
    "\n",
    "import math\n",
    "from flows import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[1.5000, 2.5000],\n         [0.0000, 0.5000]]], grad_fn=<StackBackward>)\ntensor(-0.2877, grad_fn=<SumBackward0>)\ntensor([[2.5000, 2.5000]], grad_fn=<AddBackward0>) tensor(-0.2877, grad_fn=<SumBackward0>)\ntensor([[1., 2.]], grad_fn=<SqueezeBackward1>) tensor(0.2877, grad_fn=<MulBackward0>)\n"
    }
   ],
   "source": [
    "# A flow module with a simple Triangular matrix multiply\n",
    "net = DenseTriangularFlow(2, False)\n",
    "net.w = nn.Parameter(torch.tensor([[1.5, 0.0], [2.5, 0.5]]))\n",
    "net.b = nn.Parameter(torch.tensor([1.0, -1.0]))\n",
    "x = torch.tensor([[1.0, 2.0]])\n",
    "x.requires_grad = True\n",
    "\n",
    "print(jacobian(net, x))\n",
    "print(jacobian(net, x)[0].diag().log().sum())\n",
    "\n",
    "y, log_det = net(x)\n",
    "print(y, log_det)\n",
    "x, log_det_x = net.backward(y)\n",
    "print(x, log_det_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[0.1966, 0.0000],\n         [0.0000, 0.1050]],\n\n        [[0.0452, 0.0000],\n         [0.0000, 0.0177]]], grad_fn=<StackBackward>)\ntensor(-3.8804, grad_fn=<SumBackward0>)\ntensor([[0.7311, 0.8808],\n        [0.9526, 0.9820]], grad_fn=<SigmoidBackward>) tensor([-3.8804, -7.1335], grad_fn=<SumBackward1>)\ntensor([[1.0000, 2.0000],\n        [3.0000, 4.0000]], grad_fn=<LogBackward>) tensor([3.8804, 7.1335], grad_fn=<NegBackward>)\n"
    }
   ],
   "source": [
    "# A flow module which applies the logistic sigmoid function\n",
    "net = SigmoidFlow()\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "x.requires_grad = True\n",
    "\n",
    "print(jacobian(net, x))\n",
    "print(jacobian(net, x)[0].diag().log().sum())\n",
    "\n",
    "y, log_det = net(x)\n",
    "print(y, log_det)\n",
    "x, log_det_x = net.backward(y)\n",
    "print(x, log_det_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[ 5.0867,  0.0000],\n         [ 0.0000,  9.5246]],\n\n        [[22.1468,  0.0000],\n         [ 0.0000, 56.5739]]], grad_fn=<StackBackward>)\ntensor(3.8805, grad_fn=<SumBackward0>)\ntensor([[1.0002, 2.0000],\n        [3.0006, 3.9992]], grad_fn=<LogBackward>) tensor([3.8805, 7.1332], grad_fn=<NegBackward>)\ntensor([[0.7311, 0.8808],\n        [0.9526, 0.9820]], grad_fn=<SigmoidBackward>) tensor([-3.8805, -7.1332], grad_fn=<SumBackward1>)\n"
    }
   ],
   "source": [
    "net = InverseFlow(SigmoidFlow())\n",
    "x = torch.tensor([[0.7311, 0.8808], [0.9526, 0.9820]])\n",
    "x.requires_grad = True\n",
    "\n",
    "print(jacobian(net, x))\n",
    "print(jacobian(net, x)[0].diag().log().sum())\n",
    "\n",
    "y, log_det = net(x)\n",
    "print(y, log_det)\n",
    "x, log_det_x = net.backward(y)\n",
    "print(x, log_det_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.9901, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.9901, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.3333]]], grad_fn=<StackBackward>)\ntensor(-1.8117, grad_fn=<SumBackward0>)\ntensor([[-0.6931, -0.0100,  0.0000,  0.0100,  1.0986]],\n       grad_fn=<SoftlogFlowFunctionBackward>) tensor([-1.8117], grad_fn=<SumBackward1>)\ntensor([[-1.0000, -0.0100,  0.0000,  0.0100,  2.0000]], grad_fn=<MulBackward0>) tensor([1.8117], grad_fn=<NegBackward>)\n"
    }
   ],
   "source": [
    "# A flow module that applies the element-wise function:\n",
    "# y = sign(x) log(abs(x) + 1)\n",
    "net = SoftlogFlow()\n",
    "x = torch.tensor([[-1.0, -0.01, 0.0, 0.01, 2.0]])\n",
    "x.requires_grad = True\n",
    "\n",
    "print(jacobian(net, x))\n",
    "print(jacobian(net, x)[0].diag().log().sum())\n",
    "\n",
    "y, log_det = net(x)\n",
    "print(y, log_det)\n",
    "x, log_det_x = net.backward(y)\n",
    "print(x, log_det_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[1., 0., 0., 0., 0.],\n         [0., 1., 0., 0., 0.],\n         [0., 0., 1., 0., 0.],\n         [0., 0., 0., 1., 0.],\n         [0., 0., 0., 0., 1.]]], grad_fn=<StackBackward>)\ntensor(0., grad_fn=<SumBackward0>)\ntensor([[-2., -1.,  0.,  1.,  2.]], grad_fn=<SoftsquareFlowFunctionBackward>) tensor([0.], grad_fn=<SumBackward1>)\ntensor([[-0., -0., 0., 0., 0.]], grad_fn=<DivBackward0>) tensor([-0.], grad_fn=<NegBackward>)\n"
    }
   ],
   "source": [
    "# A flow module that applies the element-wise function:\n",
    "# y = a * x + b * sign(x) * x^2\n",
    "net = SoftsquareFlow(1)\n",
    "x = torch.tensor([[-2.0, -1.0, 0.0, 1.0, 2.0]])\n",
    "x.requires_grad = True\n",
    "\n",
    "print(jacobian(net, x))\n",
    "print(jacobian(net, x)[0].diag().log().sum())\n",
    "\n",
    "y, log_det = net(x)\n",
    "print(y, log_det)\n",
    "x, log_det_x = net.backward(y)\n",
    "print(x, log_det_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[1., 0.],\n         [0., 1.]],\n\n        [[1., 0.],\n         [0., 1.]]], grad_fn=<StackBackward>)\n0.0\ntensor([[-1.,  0.],\n        [ 1.,  2.]], grad_fn=<AddBackward0>) tensor(0., grad_fn=<LogBackward>)\ntensor([[-1.,  0.],\n        [ 1.,  2.]], grad_fn=<SubBackward0>) tensor([[0.]], grad_fn=<LogBackward>)\n"
    }
   ],
   "source": [
    "# A rank-one update from the identity matrix.\n",
    "# This flow is interesting, because it is not triangular, but has a tractable jacobian determinant.\n",
    "# In order to calculate the determinant, we use numpy.linalg\n",
    "from flows import *\n",
    "import numpy\n",
    "\n",
    "net = RankOneConvolutionFlow(2, epsilon=0.0)\n",
    "# net.vT = torch.nn.Parameter(torch.tensor([[4.0, 0.25]]))\n",
    "# net.u = torch.nn.Parameter(torch.tensor([[0.5], [2.0]]))\n",
    "x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])\n",
    "x.requires_grad = True\n",
    "\n",
    "print(jacobian(net, x))\n",
    "print(math.log(numpy.linalg.det(jacobian(net, x)[0].detach().numpy())))\n",
    "\n",
    "y, log_det = net(x)\n",
    "print(y, log_det)\n",
    "x, log_det_x = net.backward(y)\n",
    "print(x, log_det_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[ 2.1055, -0.2882],\n         [ 0.5084,  0.8675]],\n\n        [[ 2.1055, -0.2882],\n         [ 0.5084,  0.8675]]], grad_fn=<StackBackward>)\n0.6795525332573441\ntensor([[-2.1055,  0.2882],\n        [ 3.1223,  1.4467]], grad_fn=<AddBackward0>) tensor(0.6796, grad_fn=<LogBackward>)\ntensor([[-1.0000e+00, -2.9802e-08],\n        [ 1.0000e+00,  2.0000e+00]], grad_fn=<SubBackward0>) tensor([[-0.6796]], grad_fn=<LogBackward>)\n"
    }
   ],
   "source": [
    "# Another test case for the rank-one convolution flow\n",
    "import numpy\n",
    "\n",
    "net = RankOneConvolutionFlow(2, epsilon=0.0)\n",
    "net.vT = torch.nn.Parameter(torch.tensor([[-1.0497, -0.4827]]))\n",
    "net.u = torch.nn.Parameter(torch.tensor([[-1.0532], [0.2746]]))\n",
    "x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])\n",
    "x.requires_grad = True\n",
    "\n",
    "print(jacobian(net, x))\n",
    "print(math.log(numpy.linalg.det(jacobian(net, x)[0].detach().numpy())))\n",
    "\n",
    "y, log_det = net(x)\n",
    "print(y, log_det)\n",
    "x, log_det_x = net.backward(y)\n",
    "print(x, log_det_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[0.2420, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.3989, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.3989, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.3989, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.2420]]], grad_fn=<StackBackward>)\ntensor(-5.5948, grad_fn=<SumBackward0>)\ntensor([[0.1587, 0.4960, 0.5000, 0.5040, 0.8413]], grad_fn=<MulBackward0>) tensor([-5.5948], grad_fn=<SumBackward1>)\ntensor([[-1.0000, -0.0100,  0.0000,  0.0100,  1.0000]], grad_fn=<AddBackward0>) tensor([5.5948], grad_fn=<MulBackward0>)\n"
    }
   ],
   "source": [
    "# A flow which applies the element-wise Normal CDF function\n",
    "net = NormalCdfFlow()\n",
    "x = torch.tensor([[-1.0, -0.01, 0.0, 0.01, 1.0]])\n",
    "x.requires_grad = True\n",
    "\n",
    "print(jacobian(net, x))\n",
    "print(jacobian(net, x)[0].diag().log().sum())\n",
    "\n",
    "y, log_det = net(x)\n",
    "print(y, log_det)\n",
    "x, log_det_x = net.backward(y)\n",
    "print(x, log_det_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(0.3504)"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# A flow which calculates the Negative Log Likelihood of the multivariate Gaussian with an identity covariance matrix.\n",
    "net = NegLogLikelihoodLoss(2)\n",
    "x = torch.tensor([[0.0, 0.], [0.3, 1.4]])\n",
    "net(x, torch.tensor(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(-0.)"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# A module which composes the given flows, converting values from (0-1) to Negative Log Likelihoods.\n",
    "net = FlowModule(\n",
    "    InverseFlow(NormalCdfFlow()),\n",
    "    NegLogLikelihoodLoss(2)\n",
    ")\n",
    "x = torch.tensor([[0.50, 0.5], [0.5, 0.50], [0.50, 0.50]])\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(2.1529)\ntensor(-2.1529)\n"
    }
   ],
   "source": [
    "# The Negative Log Likelihood loss function, tested against the Pytorch normal distribution\n",
    "net = NegLogLikelihoodLoss(2)\n",
    "dist = torch.distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "\n",
    "x = torch.tensor([[0.4, 0.5], [0.6, 0.7]])\n",
    "\n",
    "print(net.forward(x, 0.0))\n",
    "print(dist.log_prob(x).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(tensor([[0.7311, 0.8808, 0.9526]], grad_fn=<SigmoidBackward>), tensor([-6.9776], grad_fn=<AddBackward0>))\ntensor([[[0.1966, 0.0000, 0.0000],\n         [0.0000, 0.1050, 0.0000],\n         [0.0000, 0.0000, 0.0452]]], grad_fn=<StackBackward>)\n-6.97755383437876\n"
    }
   ],
   "source": [
    "# Test several flows composed, against the jacobian determinant calculated from autodiff and np.linalg.det\n",
    "import numpy as np\n",
    "\n",
    "tri = DenseTriangularFlow(3, True)\n",
    "triu = DenseTriangularFlow(3, False)\n",
    "sig = SigmoidFlow()\n",
    "net = Flows(tri, triu, sig)\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "x.requires_grad = True\n",
    "\n",
    "print(net(x))\n",
    "print(jacobian(net, x))\n",
    "print(math.log(np.linalg.det(jacobian(net, x)[0].detach().numpy())))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}